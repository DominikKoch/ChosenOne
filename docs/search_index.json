[
["index.html", "The Chosen One Identify the most suitable algorithm for your use case. Preface", " The Chosen One Identify the most suitable algorithm for your use case. Dominik Koch 2020-08-02 Preface Many data scientist always stick to their preferred algorithm for machine learning without considering alternatives. This book is a guide to identifying suitable alternatives to better meet the use case needs. Learn new algorithms that you have never used before, familiarize yourself with their advantages and disadvantages to find the Chosen One. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction WIP: Tell the background story for this book idea (everybody has its own favourite algorithmn) kaggle dominance of fancy stacked models which are not usable in real life (see netflix competition) find the best algorithm for each situation depending on several criteria Structure of the book "],
["strategy.html", "Chapter 2 Comparison Strategy 2.1 Criteria 2.2 Radar Chart", " Chapter 2 Comparison Strategy All too often, the choice of the algorithm used is made primarily on the basis of its accuracy. But there are many more features that need to be considered. Since the breakthrough of Deep Learning, the lack of interpretability of these algorithms has been a thorn in the side of many users. In addition, there are a number of other criteria that need to be considered when looking for the appropriate algorithm. Since many criteria are competing with each other, it is important to weigh up which characteristics are absolutely necessary for your use case and which ones are nice to have. 2.1 Criteria I decided to summarize these characteristics into six criteria, which should allow a good comparison of the algorithms. Accuracy in general: A high accuracy is actually always a must have criterion. However, this is sometimes achieved at the expense of other characteristics. In order to be able to classify the accuracy, benchmark tests are carried out in Chapter 4 using different data sets to ensure a fair comparison. To perform well in this category, an algorithm should perform well in different use cases and not be a one hit wonder. Since perfect accuracy is rarely possible, the evaluation is always carried out in direct comparison with the other algorithms. Speed: Especially when dealing with large amounts of data, speed can play an essential role. A distinction must be made between the time required for the model training and the time required for the scoring. Model training speed is more important if the model has to be recalibrated frequently. Scoring speed becomes important if live training is required. In batch scoring mode the speed is only important if the algorithm is too slow to process the amount of incoming data. If you only have to score a small amount of data once this characteristic can be ignored. Interpretability: In general, the simpler the model, the better the interpretability. In the best case one can see the actual effect of a feature on the prediction. This is the case for example with linear models or decision trees. More complex algorithms, on the other hand, usually only offer the option of identifying which feature has the greatest influence on the result, the so called variable importance. However, one only knows that an influence exists and does not learn about its strength and direction. Deep learning algorithms or stacked models which are pretty popular in Kaggle competitions, are usually black boxes in which the algorithms themselves do not allow conclusions about the effect of individual features. But even if the algorithm itself gives no further information about its individual feature effects, there is a selection of model agnostic methods with which one can bring light into the dark. A good source of information on this topic is the book Interpretable Machine Learning (Molnar 2019) which is available free of charge. Training Requirements: Some algorithms are accompanied by certain assumptions which must be fulfilled by the data sets. It is usually the user’s task to ensure that all these assumptions are actually met. Also the minimum number of observations required for the model training can vary greatly. We live in the Big Data age, but there are always use cases where for various reasons only a small number of observations are available. An extreme case of this can be that even more variables than observations are available. This can be a show-stopper for many algorithms. On the other hand, scalability play an increasingly important role nowadays. It is relatively easy to rent a large computing cluster but why spend extra money when there are algorithms that scale better than others? Last but not least, online learning options should be emphasized. Some algrotihms offer the possibility to improve an existing model with newly added observations without starting the training completely from scratch. Tolerance: Each data set has its own pitfalls that the algorithm must be able to deal with. Or you know the limits of the algorithm and prepare the data in advance. These pitfalls include missing data, interdependent attributes, redundant attributes and irrelevant attributes. Some algorithms can save the user a lot of time by making the data preparation step obsolete. However, one should always keep in mind that every algorithm is grateful to receive clean data as stated in the “garbage in, garbage out” concept. Simplicity: Finally, one should consider the complexity of simplicity of an algorithm. The more hyperparameters an algorithm has, the more precisely it can be adopted to the users specific use case. However, this requires a deep knowledge of the algorithm itself or its implementation. Different implementations of the same concept provide the user different degrees of freedom when calibrating the algorithm. At this point, Citizen Data Scientists who want to test a new fancy algorithm they have stumbled upon by reusing code snippets from the tutorial or stack overflow often fail. Without worrying about the hyperparameters they will most likely be surprised by bad results. To ensure a fair comparison of the algorithms, I will take a closer look at the following characteristics and evaluate them for each algorithm on a 5 point scale. 2.2 Radar Chart In order to be able to compare the myriad of machine learning algorithms better, the mentioned characteristics are illustrated in a radar chart. The better an algorithm scores in a category, the further out the point is visualized. A perfect algorithm would therefore completely surround the radar chart. Figure 2.1: Algorithm comparison example. References "],
["algorithms.html", "Chapter 3 Machine Learning Algorithms 3.1 Regression 3.2 Classification", " Chapter 3 Machine Learning Algorithms WIP: hyperparameters fill algorithm sections This chapter lists all kinds of regression and classification algorithms, that I want to compare in this book. You will find algorithms from both the supervised and unsupervised faction. In addition to the names of the algorithms I introduce short names, which are used in the visualizations. In addition, I give a brief overview of the most important hyperparameters of each algorithm, which should be given the most attention during the hyperparameter tuning step. Click on the short name to go directly to the corresponding section. For further information about the package, click on the package name. Table 3.1: List of Machine Learning Algorithms Short name Model name Type Package Hyperparameters cforest Random Forest Based on Conditional Inference Trees regression, classification party todo ctree Conditional Inference Trees regression, classification party todo gam Generalized Additive Regression regression mgcv todo gbm Gradient Boosting Machine regression, classification gbm todo glm Generalized Linear Regression regression stats todo glmnet Lasso and Elastic-Net Regularized Generalized Linear Model regression, classification glmnet todo knn k-Nearest Neighbor classification class todo lda Linear Discriminant Analysis classification MASS todo logreg Logistic Regression classification stats todo nbayes Naive Bayes classification e1071 todo ranger Random Forest regression, classification ranger todo rf Random Forest regression, classification randomForest todo rpart Decision Tree regression, classification rpart todo svm Support Vector Machines regression, classification e1071 todo xgboost Extreme Gradient Boosting regression, classification xgboost todo For each of the listed algorithms I will create the radar chart with the rating after a short theory section. Futhermore, all advantages and disadvantages are listed, which go beyond the criteria shown in the radar chart. Last but not leas I will highlight the most important hyperparameters, so that a more efficient hyperparameter tuning is possible. 3.1 Regression … 3.1.1 glm - Generalized Linear Regression 3.1.1.1 Basic Theory 3.1.1.2 Hyperparamters 3.1.1.3 Advantages / Disadvantages 3.2 Classification … "],
["benchmark.html", "Chapter 4 Benchmark", " Chapter 4 Benchmark WIP: Describe the used Datasets Benchmark Szenarios different training size different variable types interdependent/redundant/irrelevant attributes n&lt;&lt;p case missing data Benchmark Code "],
["dashboard.html", "Chapter 5 Interactive Dashboard", " Chapter 5 Interactive Dashboard WIP: Filter to find the most suited algorithm for the use case at hand "],
["references.html", "References", " References "]
]
